---
title: "Estadística Aplicada 3 - Tarea 2"
lang: es
author: 
  -Marcelino
  -David
  -Daniela
date: today

format:
  html:
    page-layout: full
    embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA, tidy.opts=list(width.cutoff=40))
```

```{r, message=FALSE, warning=FALSE}
#Cargamos paquetes
library(tidymodels)
library(discrim)
library(corrr)
library(paletteer)
library(MASS)
library(dslabs)
library(tidyr)
library(openxlsx)

# Cargamos bases de datos
```

# Ejercicio 1

Consider the color-stimuli experiment outlined in Section 13.2.1. The similarity ratings are given in the file color-stimuli on the books's website. Carry out a clasical scaling of the data and show that the solution is a "color circle" ranging from violet (434 $m\mu$) to blue (472 $m\mu$) to green (504 $m\mu$) to yellow (584 $m\mu$) tto red  (674 $m\mu$). Compare the solution to the nonmetric scaling solution given in Figure 13.3.

En primer lugar realizamos el `clásico MDS` con la función `cmdscale` del paquete `stats` de R. Y en un principio no obtuvimos los resultados esperados, esto pasó porque la base de datos de `color-stimuli` presenta una matriz de disimilaridad y no de proximidad, la cual por alguna extraña razón es la que utiliza el paquete `cmdscale` para realizar el MDS clásico. Con lo cual realizando los ajustes necesarios obtuvimos lo siguiente mostrados en la figura 1.

```{r}
# Cargamos base de datos
data1 <- read.xlsx("color_stimuli.xlsx")
matrix <- 1- as.matrix(data1)
diag1 <- diag(rep(1,14))
matrix <- matrix - diag1
# Clasical scaling
classical <- cmdscale(matrix, k=2, eig=TRUE, add=TRUE)

hexcodes <- c('#2800ff', 
              '#0028ff',
              '#0092ff',
              '#00b2ff',
              '#00ffff',
              '#00ff61',
              '#77ff00',
              '#b3ff00',
              '#fff200',
              '#ffbe00',
              '#ff9b00',
              '#ff5700',
              '#ff0000',
              '#e50000'
              )

# Mapear longitudes de onda a colores 

wavelengths <- c(434, 445, 465, 472, 490, 504, 537, 555, 584, 600, 610, 628, 651, 674)

# Crear un dataframe para ggplot
df <- data.frame(classical$points, Wavelength = wavelengths, Color = hexcodes)

# Crear la gráfica
ggplot(df, aes(x = X1, y = X2, color = Color, label = Wavelength)) +
  geom_point() +
  geom_text(vjust = -1) +
  scale_color_identity() +
  labs(title = "Classical Scaling", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()

```

Ahora bien, para poder comparar los resultados obtenidos con el MDS clásico contra los resultados del la `figura 13.3` del libro tenemos la siguiente imagen.

![](non_metric_MDS.png)

Y notamos que con el clásico logramos el mismo efecto de círculo de colores esperado y observado con el no métrico. De hecho, realmente los resultados son muy similares obtenidos por los dos métodos si solo nos fijamos en la estructura de los colores sin importar rotaciones o escalas. Claro que hay ligeras torciones observadas en el círculo de colores del clásico contra el no paramétrico, pero no son nada significativas. Como conclusión final, hace mucho sentido lo obtenido por el MDS clásico y no paramétrico, ya que los colores muy similares entre sí están más cerca, mientras que los colores muy diferentes están más lejos. 

# Ejercicio 2

Generate a random sample of size $n=100$ from a three dimensional Gaussian distribution, where one of the variables has very high variance (relative to the other two). Carry out PCA on these data using the covariance matri and the correlation matrix. In each case, find the eigenvalues and eigenvectors, draw the scree plot, compute the PC scores, and plot all pairwise PC scores in a matrix plot. Compare results.

```{r}
# Load necessary libraries
library(MASS)
library(ggplot2)
#library(GGally)

# Set seed for reproducibility
set.seed(123)

# Generate random sample
n <- 100
mu <- c(0, 0, 0)
sigma <- matrix(c(1, 0.2, 0.5, 0.2, 5, 0.3, 0.5, 0.3, 1), nrow = 3)
data <- mvrnorm(n, mu, sigma)

# Perform PCA using covariance matrix
pca_cov <- prcomp(data, scale. = FALSE)

# Perform PCA using correlation matrix
pca_cor <- prcomp(data, scale. = TRUE)

# Extract eigenvalues and eigenvectors
eigenvalues_cov <- pca_cov$sdev^2
eigenvectors_cov <- pca_cov$rotation

eigenvalues_cor <- pca_cor$sdev^2
eigenvectors_cor <- pca_cor$rotation

# Scree plots
par(mfrow = c(1, 2))
plot(1:3, eigenvalues_cov, type = "b", main = "Scree Plot (Covariance Matrix)", xlab = "Principal Component", ylab = "Eigenvalue")
plot(1:3, eigenvalues_cor, type = "b", main = "Scree Plot (Correlation Matrix)", xlab = "Principal Component", ylab = "Eigenvalue")

# Compute PC scores
scores_cov <- predict(pca_cov)
scores_cor <- predict(pca_cor)

# Matrix plot of pairwise PC scores
pairs(data.frame(scores_cov), main = "Pairwise PC Scores (Covariance Matrix)")
pairs(data.frame(scores_cor), main = "Pairwise PC Scores (Correlation Matrix)")

```

Podemos observar del Scree Plot que la explicación de varianza para el PCA medido con covarianza es mayor para el primer componente, formando allí el codo. Mientras tanto, el PCA de correlación no muestra codo sino una línea recta, con lo cual no es posible medir bien qué componente principal eliminar al ser todos significativos.

Sumado a esto, en las gráficas de Pairwise PCA Score, podemos ver que la distribución de datos para Covarianza son más dispersos y con más caos, lo cual es más informativo de cuáles componentes son informativas para representar los datos. Mientras tanto, los Pairwise PCA Score de correlación presentan menos dispersión debido a su eigenvalor más pequeño, su menor sesgo en representación y por su unicidad (correlación quita la magnitud de varianza de la normal multivariada). El puntaje de covarianza de la Gaussiana multivariada se ve afectada en el PCA Analysis por la mayor magnitud de varianza de la distribución en su segunda dimensión $Var(X_2)=5$, lo cual genera datos más sesgados y centrados a la segunda variable, lo que a su vez pesa mucho en la varianza total de todos los datos, su dispersión, y el Scree Plot. La desviación del Scree Plot en el codo es principalmente a esto, puesto que en el de correlación no hay codo y la gráfica es una línea recta, lo cual indica que cada componente tiene casi la misma importancia al estandarizarse la varianza a magnitud unitaria.





# Ejercicio 3

En esta tabla de datos, debemos dividir los datos de tortugas macho y tortugas hembra por dimensiones de su caparazón (length, width, height). Para esto, estimaremos el vector de medias, la matriz de covarianzas, los eigenvalores y eigenvectores de la matriz. Después se hará una prueba de PCA para explicar la composición de los datos, y a partir de estos, estimar el volumen de caparazones y comparar volúmenes entre machos y hembras.

Primero, volvemos logartimo natural los datos y separamos en dos grupos

```{r}
library(ggplot2)
library(dplyr)
library(logisticPCA)

turtles <- read.xlsx("turtles.xlsx")
turtles <- turtles[1:4]
turtles$length <- log(turtles$length)
turtles$width <- log(turtles$width)
turtles$height <- log(turtles$height)


turtlesF <- turtles[turtles$sex=="f ",]
turtlesM <- turtles[turtles$sex=="m ",]

```

Luego, estimamos las medias y covarianzas de ambos grupos

```{r}
#Mean Vector
meanF <- colMeans(turtlesF[2:4])
meanM <- colMeans(turtlesM[2:4])
#Covariate Matrix
covF <- cov(turtlesF[2:4])
covM <- cov(turtlesM[2:4])

meanF
covF

meanM
covM

```


Esta información sirve para distinguir media logarítmica y varianzas en los datos de las medidas para hembras (F) y machos (M).

Luego, calculamos los eigenvalores y eigenvectores. Esto nos da:

```{r}
# Calculate Eigenvalues and Eigenvectors
eigen_resF <- eigen(covF)
eigen_resM <- eigen(covM)

# Extract Eigenvalues and Eigenvectors
eigenvaluesF <- eigen_resF$values
eigenvectorsF <- eigen_resF$vectors
eigenvaluesM <- eigen_resM$values
eigenvectorsM <- eigen_resM$vectors

eigenvaluesF
eigenvectorsF
eigenvaluesM
eigenvectorsM
```

Obtenido esto, ahora podemos hacer una prueba de PCA para determinar: 

1. Independencia y maximizar varianza explicada
  
2. Determinar la varianza explicada
   
3. Describir el volúmen de los caparazones

Primero haremos las pruebas:

```{r}
#PCA
#calculate principal components
resultsF <- prcomp(turtlesF[2:4], scale = TRUE)
resultsM <- prcomp(turtlesM[2:4], scale = TRUE)


#reverse the signs
resultsF$rotation <- -1*resultsF$rotation
resultsM$rotation <- -1*resultsM$rotation

#display principal components
resultsF$rotation
resultsM$rotation

#Plot the graph of PCA
biplot(resultsF, scale = 0)
biplot(resultsM, scale = 0)
```

Notemos que PC1 es casi una expresión lineal del vector $(1,1,1)$, con lo cual, PC1 se puede considerar un proxy para estimar el volúmen de caparazones.

Sumado a esto, también tenemos la "Gráfica de Codo" y la explicación de varianza:

```{r, warning=FALSE}
#calculate total variance explained by each principal component
var_explainedF = resultsF$sdev^2 / sum(resultsF$sdev^2)
var_explainedM = resultsM$sdev^2 / sum(resultsM$sdev^2)

var_explainedF
var_explainedM

#create scree plot
qplot(c(1:3), var_explainedF) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot (Female Turtles)") +
  ylim(0, 1)
qplot(c(1:3), var_explainedM) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot (Male Turtles)") +
  ylim(0, 1)

```

Más del 95% de la varianza lo explica el PCA1. Por tanto, para describir el volumen podemos usar casi a la perfección el PCA1 y hacer una simple regresión para diferenciar caparazones de machos y hembras en sus volúmenes.

Por último, creamos un Dataframe con el PCA1 y los volúmenes calculados, los cuales se usará con la fórmula $Vol = (\pi/6) * \exp(\text{loglength} + \text{logwidth} + \text{logheight})$

```{r}
#Volumen y regresión

turtles$Volume <- pi/6*exp(turtles$length+turtles$width+turtles$height)
ggplot(turtles, aes(x = height, y = Volume, color = sex)) +
  geom_point() +
  labs(title = "Scatter Plot of Female and Male Turtles", 
       x = "X-axis Label(height)", y = "Y-axis Label(Volume)") +
  theme_minimal()



#PCA Data
pca_scores <- as.matrix(turtlesF[2:4]) %*% resultsF$rotation

# Extract the first principal component (PCA1)
pca1 <- pca_scores[, 1]

# Create a new data frame with only PCA1
df_pca1 <- data.frame(PCA1 = pca1)
df_pca1
df_pca1$Volume <-pi/6*exp(turtlesF$length+turtlesF$width+turtlesF$height)




#PCA Data
pca_scores2 <- as.matrix(turtlesM[2:4]) %*% resultsM$rotation

# Extract the first principal component (PCA1)
pca2 <- pca_scores2[, 1]

# Create a new data frame with only PCA1
df_pca2 <- data.frame(PCA1 = pca2)
df_pca2
df_pca2$Volume <-pi/6*exp(turtlesM$length+turtlesM$width+turtlesM$height)



plot <- ggplot()+
  geom_point(data=df_pca1, aes(x = PCA1, y = Volume), color="red")+
  geom_point(data=df_pca2, aes(x = PCA1, y = Volume),color="blue") +
  labs(title = "Scatter Plot of Female and Male Turtles", 
       x = "X-axis Label", y = "Y-axis Label") +
  theme_minimal()
print(plot)

```

Con esto podemos ver una fuerte correlación exponencial entre el PCA1 de cada sexo y su volúmen de caparazón. Con esto queda descrito el problema de la simplificación de los datos a una sola variable explicatoria por medio de PCA Analysis.



# Ejercicio 4

Let X and Y be random variables with a joint distribution function given by

$$
H(x,y)=(1+e^{-x}+e^{-y})^{-1}
$$

for all $x$, $y$ in $\overline{R}$.

a. Show that X and Y have standard (univariate) logistic distributions, i. e.,

$$
F(x)=(1+e^{-x})^{-1} \space \space \space G(y)=(1+e^{-y})^{-1}
$$

b. Show that the copula of X and Y is the copula given by (2.3.4) in Example 2.8.

**Solution:**

a) Buscaremos las distribuciones marginales:

$F(x) = \lim_{y \to \infty} \left(1+e^{-x}+ e^{-y}\right)^{-1} = \left(1+e^{-x}\right)^{-1}$

$G(y) = \lim_{x \to \infty} \left(1+e^{-x}+ e^{-y}\right)^{-1} = \left(1+e^{-y}\right)^{-1}$

$\text{Q.E.D.}$

b) La cópula de $X$ e  $Y$  está dada por:

$C(u, v) = \frac{H(F^{-1}(u), G^{-1}(v))}{uv}$

Para encontrar la copula, necesitamos las funciones inversas de las distribuciones marginales $F^{-1}(u)$ y $G^{-1}(v)$.

Para la distribución logística estándar, las funciones inversas son:

$F^{-1}(u) = -\ln\left(\frac{1}{u} - 1\right)$
$G^{-1}(v) = -\ln\left(\frac{1}{v} - 1\right)$

Sustituyendo estas funciones inversas en la fórmula de la copula:

$C(u, v) = \frac{H\left(-\ln\left(\frac{1}{u} - 1\right), -\ln\left(\frac{1}{v} - 1\right)\right)}{uv}$

$= \left(1+e^{\ln\left(\frac{1}{u} - 1\right)} + e^{\ln\left(\frac{1}{v} - 1\right)}\right)^{-1} / uv$

$= \left(1+ \frac{1}{u} -1+ \frac{1}{v} -1\right)^{-1} / uv$

$= \left(\frac{1}{u} + \frac{1}{v} -1\right)^{-1} / uv$

$= \frac{1}{\left(\frac{uv}{u} + \frac{uv}{v} - uv \right)}$

$= \frac{1}{(u+v-uv)}$


Que era lo que queríamos demostrar.


# Ejercicio 5

(A) Show that the following algorithm (Devroye 1987) generates random variables  (x,y) from the Marshall-Olkin bivariate exponential distribution with parameters $\lambda_{1}$, $\lambda_{2}$ and $\lambda_{1,2}$. 

    1.-Generate three independent uniform (0,1) variates $r, s, t$

    2.-Set $x= min\left(\frac{-ln r}{\lambda_{1}},\frac{-ln t}{\lambda_{1,2}}\right)$ , $y= min\left(\frac{-ln s}{\lambda_{2}},\frac{-ln t}{\lambda_{1,2}}\right)$

    3.- The desired pair is $(x,y)$.

Primero notemos que si $Z$ es una variable aleatoria con distribución $Exp(\lambda)$, entonces su distribución acumulada es $F_{Z}(z)=1-e^{-\lambda z}$ y por lo tanto $F_{Z}^{-1}(u)=-\frac{ln(1-u)}{\lambda}$.

Con lo cual por el teorema de la transformada inversa, si $U$ es una variable aleatoria con distribución uniforme en $(0,1)$, entonces $F_{Z}^{-1}(U)=-\frac{ln(1-U)}{\lambda}$ tiene distribución $Exp(\lambda)$. 

Por lo tanto con el algoritmo, tenemos que $Z_{1}=-\frac{ln(1-r)}{\lambda_{1}}$, $Z_{2}=-\frac{ln(1-s)}{\lambda_{2}}$ y $Z_{3}=-\frac{ln(1-t)}{\lambda_{1,2}}$ tienen distribución $Exp(\lambda_{1})$, $Exp(\lambda_{2})$ y $Exp(\lambda_{1,2})$ respectivamente. Las cuales son exponenciales independientes porque son transformaciones individuales de variables que igualmente son independientes. Y de acuerdo con la definición del libro, $X=min(Z_{1},Z_{3})$ y $Y=min(Z_{2},Z_{3})$ tienen distribución Marshall-Olkin bivariada con parámetros $\lambda_{1}$, $\lambda_{2}$ y $\lambda_{1,2}$.

(B) Show that $u=e^{-(\lambda_{1}+\lambda_{1,2})x}$ and $v=e^{-(\lambda_{2}+\lambda_{1,2})y}$ are uniform (0,1) variates whose joint distribution is the Marshall-Olkin copula given by 3.1.3.

En este caso necesitamos demostrar el siguiente lema. Sean $X$ y $Y$ variables aleatorias independientes con distribución $Exp(\lambda)$ y $Exp(\mu)$ respectivamente. Entonces $Z=\text{min}(X,Y)\sim Exp(\lambda + \mu)$. 

Para demostrarlo notemos que $P(Z\geq z)=P(X\geq x, Y \geq y)$ y como son independientes obtenemos el siguiente desarrollo:

\begin{align*}
P(Z\geq z)=P(X\geq z, Y\geq z) &= P(X\geq z)P(Y\geq z)\\
 &= e^{-\lambda z }e^{-\mu z} \\
 &= e^{-(\lambda+\mu)z} \\
\end{align*}

Es decir, $Z\sim Exp(\lambda + \mu)$.

Por lo tanto, sean $Z_{1}$, $Z_{2}$ y $Z_{3}$ las variables aleatorias independientes con distribución $Exp(\lambda_{1})$, $Exp(\lambda_{2})$ y $Exp(\lambda_{1,2})$ respectivamente definidas como en el anterior inciso. Entonces $X=min(Z_{1},Z_{3})$ y $Y=min(Z_{2},Z_{3})$ tienen distribución $Exp(\lambda_{1}+\lambda_{1,2})$ y $Exp(\lambda_{2}+\lambda_{1,2})$ respectivamente. 

Con lo cual, $u=e^{-(\lambda_{1}+\lambda_{1,2})x}$ y $v=e^{-(\lambda_{2}+\lambda_{1,2})y}$ son uniformes en $(0,1)$ utilizando el mismo argumento que en el inciso anterior del teorema de la transformada inversa, aplicada a las funciones de distribución de $X$ y $Y$ acumuladas. Ahora solo notemos que la distribución conjunta de $u$ y $v$ es la siguiente:

\begin{align*}
S_{U,V}(u,v) &= P(e^{-(\lambda_{1}+\lambda_{1,2})X} \geq u, e^{-(\lambda_{2}+\lambda_{1,2})Y} \geq v) \\
 &= P(X\geq S_{X}^{-1}(u), Y\geq S_{Y}^{-1}(v)) \\
 &= P(X\geq -\frac{ln(u)}{\lambda_{1}+\lambda_{1,2}}, Y\geq -\frac{ln(v)}{\lambda_{2}+\lambda_{1,2}}) \\
 &= \textbf{exp}\{-\lambda_{1}S_{X}^{-1}(u) - \lambda_{2}S_{Y}^{-1}(v) - \lambda_{1,2}\text{max}(S_{X}^{-1}(u),S_{Y}^{-1}(v)) \}\\
 &= \textbf{exp}\{-(\lambda_{1}+\lambda_{1,2})S_{X}^{-1}(u) - (\lambda_{2}+\lambda_{1,2})S_{Y}^{-1}(v) + \\
 &  \lambda_{1,2}\text{min}(S_{X}^{-1}(u),S_{Y}^{-1}(v)) \}\\
 &=S_{X}(S_{X}^{-1}(u)) S_{Y}(S_{Y}^{-1}(v)) \text{min}(\textbf{exp}(\lambda_{1,2}(S_{X}^{-1}(u))),\textbf{exp}(S_{Y}^{-1}(v))) \\
 &= u v \text{min}(u^{-\alpha}, v^{-\beta})\\
 \end{align*}

donde $\alpha=\frac{\lambda_{1,2}}{\lambda_{1}+\lambda_{1,2}}$ y $\beta=\frac{\lambda_{1,2}}{\lambda_{2}+\lambda_{1,2}}$.

Y como pudimos observar, la distribución conjunta de $u$ y $v$ es la misma que la cópula de supervivencia dada en el libro. Además, notemos que $u=e^{-(\lambda_{1}+\lambda_{1,2})x}$ y $v=e^{-(\lambda_{2}+\lambda_{1,2})y}$ son uniformes en $(0,1)$ utilizando el mismo argumento que en el inciso anterior del teorema de la transformada inversa, aplicada a las funciones de distribución de $X$ y $Y$ acumuladas